{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Final Project </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __ We first download load the data __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474432\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_reviews = pd.DataFrame()\n",
    "file_list = os.listdir(\"../yelp_dataset/reviews_filter.json\")\n",
    "\n",
    "for file in file_list:\n",
    "    # loading the reviews into a dataframe\n",
    "    df = pd.read_json(\"../yelp_dataset/reviews_filter.json/\" + file, lines=True) #specify that json has multiple lines\n",
    "    df_reviews = pd.concat([df_reviews, df])\n",
    "\n",
    "# removing rows containing NaN value and reaffecting index to the df\n",
    "df_reviews = df_reviews.dropna() \n",
    "df_reviews = df_reviews.reset_index(drop=True)\n",
    "\n",
    "print(len(df_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108521\n"
     ]
    }
   ],
   "source": [
    "# loading the business into a dataframe\n",
    "df_business = pd.DataFrame()\n",
    "file_list = os.listdir(\"../yelp_dataset/open_business.json\")\n",
    "    \n",
    "for file in file_list:\n",
    "    # loading business into a dataframe\n",
    "    df = pd.read_json(\"../yelp_dataset/open_business.json/\" + file, lines=True) #specify that json has multiple lines\n",
    "    df_business = pd.concat([df_business, df])\n",
    "\n",
    "# removing rows containing NaN value and reaffecting index to the df\n",
    "df_business = df_business.dropna() \n",
    "df_business = df_business.reset_index(drop=True)\n",
    "\n",
    "print(len(df_business))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews keys :  ['business_id', 'date', 'review_id', 'stars', 'text', 'user_id']\n",
      "Business keys :  ['address', 'attributes', 'business_id', 'categories', 'city', 'hours', 'is_open', 'latitude', 'longitude', 'name', 'neighborhood', 'postal_code', 'review_count', 'stars', 'state']\n"
     ]
    }
   ],
   "source": [
    "print(\"Reviews keys : \", list(df_reviews.keys()))\n",
    "print(\"Business keys : \", list(df_business.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __ The idea is now to filter the business dataframe in order to select only the business that have reviews__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8191\n",
      "7799\n"
     ]
    }
   ],
   "source": [
    "list_business = list(df_reviews.business_id.unique())\n",
    "print(len(list_business))\n",
    "\n",
    "# selecting only the wanted business\n",
    "df_business = df_business[df_business['business_id'].isin(list_business) == True]\n",
    "df_business = df_business.reset_index(drop=True)\n",
    "print(len(df_business))\n",
    "\n",
    "# strange thing : the two following lists do not have the same length\n",
    "# print(len(list_business) - len(df_business))\n",
    "# print(list(set(list_business) - set(list(df_business.business_id.values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "labMT = pd.read_csv('data/s001.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function computing the sentimental value of a text\n",
    "def computeSentiment(text):\n",
    "    happiness_average, text = 0, word_tokenize(text)\n",
    "    words = {word: text.count(word) for word in set(text)}\n",
    "    \n",
    "    for word in words:\n",
    "        val_happiness = labMT[labMT[\"word\"] == word].happiness_average.values\n",
    "        # checking if the word is in labMT or not\n",
    "        if len(val_happiness)>0:\n",
    "            happiness_average += float(val_happiness[0]) * words[word] # taking into account the nb of occurences of the word\n",
    "            \n",
    "    return happiness_average / len(text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __Now creating a dictionnary where keys are business id and value a list of all the comments related to this business. In order to compute the rating of each business. And another one where keys are business id and values the sentiment value__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_reviews = {business: [] for business in list_business}\n",
    "\n",
    "for i in range(len(df_reviews)):\n",
    "    comment = df_reviews.loc[i].text\n",
    "    business = df_reviews.loc[i].business_id\n",
    "    \n",
    "    dic_reviews[business].append(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __ Use the first cell to create a json file containing as a key the business id and as value the rate of the busines. Use the second one to load the data from the file.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "dic_rate = {}\n",
    "\n",
    "for elt in list(dic_reviews.keys()):\n",
    "    dic_rate[elt] = mean([computeSentiment(text) for text in dic_reviews[elt]])\n",
    "\n",
    "with open(\"dic_rate.json\",'w') as file:\n",
    "    json.dump(dic_rate, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dic_rate.json\",'r') as file:\n",
    "    dic_rate = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing positions on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering business in order to get business in a small area\n",
    "df_business[\"latitude\"] = pd.to_numeric(df_business[\"latitude\"]) # changing type of the column to float\n",
    "df_business[\"longitude\"] = pd.to_numeric(df_business[\"longitude\"])\n",
    "\n",
    "# df_business = df_business[(df_business[\"longitude\"] > -90) & (df_business['longitude'] < -85) &(df_business['latitude'] < 42)]\n",
    "# df_business = df_business.reset_index(drop=True)\n",
    "# print(len(df_business))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use either one of the following locations :\n",
    "<li> First one gets all business locations </li>\n",
    "<li> Second one gets only the locations of the business which sentimental mark is computed</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list containing all localisations of business\n",
    "locations = []\n",
    "for i in range(len(df_business)):\n",
    "    locations.append((df_business.loc[i].latitude, df_business.loc[i].longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list containing the localisations of all the analyzed business\n",
    "locations = {}\n",
    "for business in list(dic_rate.keys()):\n",
    "    try:\n",
    "        locations[business] =((df_business[df_business['business_id'] == business].latitude.values[0], df_business[df_business['business_id'] == business].longitude.values[0]))\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7799\n",
      "7799\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# here we compute the color of each marker (business) regarding their final mark\n",
    "max_rate = max(dic_rate.values())\n",
    "med_rate = statistics.median(dic_rate.values())\n",
    "min_rate = min(dic_rate.values())\n",
    "\n",
    "def aggregateColors(rate):\n",
    "    if rate <= med_rate:\n",
    "        x = int(255*(rate - min_rate)/(med_rate - min_rate))\n",
    "        return (255, x, 0, 0.8)\n",
    "    else:\n",
    "        x = int(255*(rate - med_rate)/(max_rate - med_rate))\n",
    "        return (255 - x, 255, 0, 0.8)\n",
    "\n",
    "colors = [aggregateColors(dic_rate[elt]) for elt in list(locations.keys())]\n",
    "print(len(locations))\n",
    "print(len(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmaps\n",
    "gmaps.configure(api_key='API_KEY')\n",
    "\n",
    "place_focus = (33.448376, -112.074036) #focusing on phenix\n",
    "fig = gmaps.figure(\n",
    "        center = place_focus,\n",
    "        zoom_level = 10)\n",
    "symbols = gmaps.symbol_layer(\n",
    "        list(locations.values()), \n",
    "        fill_color=colors, \n",
    "        stroke_color=colors)\n",
    "fig.add_layer(symbols)\n",
    "\n",
    "# hm = gmaps.heatmap_layer(locations, weights=rate, max_intensity=max(rate), point_radius=5.0)\n",
    "# fig.add_layer(hm)\n",
    "\n",
    "# fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to operate business categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __Changing the type of the column categories to a list__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.categories = df_business.categories.astype(list)\n",
    "df_business.categories = df_business['categories'].map(lambda a: a.split(\",\")) # spliting each element of row into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __Finding which categories are the most used__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returning a dictionnary with key string and value the nb of occurence\n",
    "def countingOccurencesListOfLists(list_of_lists):\n",
    "    flattened = [val.lstrip() for sublist in list_of_lists for val in sublist] # flattening the list of lists\n",
    "    dic = {key : 0 for key in flattened}\n",
    "    for row in list_of_lists:\n",
    "        for elt in row:\n",
    "            dic[elt.lstrip()] += 1\n",
    "            \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# sorting the tuples according to their nb of occurences\n",
    "a = countingOccurencesListOfLists(list(df_business.categories.values))\n",
    "a = sorted(a.items(), key=lambda item: (item[1], item[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __Filter the business by a list of categories__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteringItems = [elt[0] for elt in a[133:143]]\n",
    "\n",
    "def filterDfByListElement(filteringList, df, column):\n",
    "    indexToDrop = []\n",
    "    for i in range(len(df)):\n",
    "        categories = df.loc[i][column]\n",
    "        if len(set(categories) & set(filteringList)) == 0:\n",
    "            indexToDrop.append(i)\n",
    "            \n",
    "    df = df.drop(indexToDrop)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network of elite users with friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_elite = pd.DataFrame()\n",
    "file_list = os.listdir(\"../yelp_dataset/elite_users_with_friends.json\")\n",
    "\n",
    "for file in file_list:\n",
    "    # loading the reviews into a dataframe\n",
    "    df = pd.read_json(\"../yelp_dataset/elite_users_with_friends.json/\" + file, lines=True) #specify that json has multiple lines\n",
    "    df_elite = df_elite.append(df, ignore_index=True)\n",
    "    \n",
    "df_elite = df_elite.dropna() \n",
    "df_elite = df_elite.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ __Setting 'friend' and 'elite' columns as list__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elite.elite = df_elite.elite.astype(list)\n",
    "df_elite.elite = df_elite['elite'].map(lambda a: a.split(\",\")) # spliting each element of row into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_2010 = filterDfByListElement(['2008'], df_elite, 'elite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "\n",
    "# for i in range(len(elite_2010)):\n",
    "#     print(i)\n",
    "#     G.add_node(df_elite['user_id'][i])\n",
    "#     friends_list = df_elite['friends'][i]\n",
    "#     for friend in friends_list:\n",
    "#         if list(df_elite['user_id']).count(friend) > 0:\n",
    "#             G.add_node(friend)\n",
    "#             G.add_edge(df_elite['user_id'][i], friend)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Elite users with Elite friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-022f508d1d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark_df_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../yelp_dataset/yelp_academic_dataset_user.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(spark);\n",
    "\n",
    "spark_df_users = spark.read.json(\"../yelp_dataset/yelp_academic_dataset_user.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_users = spark_df_users.where(\"friends != 'None' AND elite != 'None'\")\n",
    "spark_df_users = spark_df_users.drop(\"average_stars\", \"compliment_cool\", \"compliment_cute\", \"compliment_funny\",\n",
    "                                     \"compliment_hot\", \"compliment_list\", \"compliment_more\", \"compliment_note\",\n",
    "                                     \"compliment_photos\", \"compliment_plain\", \"compliment_profile\",\n",
    "                                     \"compliment_writer\", \"cool\", \"funny\", \"useful\", \"yelping_since\")\n",
    "\n",
    "spark_df_users = spark_df_users.withColumn(\"friends\", split(col(\"friends\"), \"\\\\|\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
